{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFPTAsjlKUVmC/rtyKdn/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walukas1/DeepDiveDSProjects/blob/main/IDA_function_Walukas_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UHtMcVT7P5lv"
      },
      "outputs": [],
      "source": [
        "def IDA(df, target,null_upper_lim, null_lower_lim):\n",
        "  ''' This function returns the following information after inputting the data frame and identifying the target column where df must be a data frame and target must be a string name of column\n",
        "null_upper_lim in decimal percent (0.4 = 40%) defines the threshhold of null data in feature in which more than will suggest to delete the column (think of better wording later)\n",
        "null_lower_lim in decimal percent (0.02 = 2%) defines the threshhold of null data in a fueature in which less than it will suggest columns to delete rows in it that has nulls\n",
        "\n",
        "Returns dict_keys(['DF rows', 'DF Columns', 'Total Nulls', 'Duplicate rows', 'meta_df', 'Target Name', 'Target count', 'Target Nulls', 'Target type', 'Drop columns', 'Drop rows'])\n",
        "'''\n",
        "\n",
        "\n",
        "#Check dataframe for size, shape, nulls and duplicates\n",
        "  rows, columns = df.shape\n",
        "  df_nulls = df.isnull().sum().sum()\n",
        "  duplicates = df.duplicated().sum()\n",
        "  meta_dict = {'DF rows' : rows, 'DF Columns' : columns, 'Total Nulls': df_nulls, 'Duplicate rows':duplicates}\n",
        "  # print('--- Features Type, Uniques and Null Counts ---')\n",
        "  meta_dict['meta_df'] = pd.concat([df.dtypes, df.nunique(), df.isnull().sum()], keys = ['Dtype', 'Uniques','Null Sum'], axis = 1)\n",
        "\n",
        "# # Check target data\n",
        "  target_nulls = df[target].isnull().sum()\n",
        "  target_count = df[target].count()\n",
        "  target_type = type(df[target][1])\n",
        "  meta_dict.update([('Target Name',target),('Target count', target_count),('Target Nulls',target_nulls), ('Target type',target_type)])\n",
        "\n",
        "  # print('--- Target Describe and Histogram ---')\n",
        "  # print(df[target].describe().transpose())\n",
        "  # target_plt = plt.hist(df[target], bins=20, label = target)\n",
        "  # plt.title(target)\n",
        "  # meta_dict['Target plt'] = target_plt\n",
        "\n",
        "#Suggest columns and rows to drop based on nulls\n",
        "  #Find location of nulls missing {null_upper_lim}% of data set\n",
        "  filter_c = df.isnull().sum() > rows * null_upper_lim\n",
        "  df_nulls = df.isnull().sum()\n",
        "  drop_columns = df_nulls[filter_c]\n",
        "  #Categories with only a few nulls ({null_lower_lim} % mising threshhold)\n",
        "  filter_r = (df.isnull().sum() > 0) & (df.isnull().sum() < rows * null_lower_lim)\n",
        "  drop_rows = df_nulls[filter_r]\n",
        "  meta_dict.update([('Drop columns',drop_columns),('Drop rows',drop_rows)])\n",
        "  return(meta_dict)"
      ]
    }
  ]
}